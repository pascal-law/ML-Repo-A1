\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\newcommand{\Value}{\mathord{\it value}}

\newcommand{\fib}{\textsf{\sc Fib}}
\newcommand{\FB}{\textsf{\sc FibBieber}}
\newcommand{\Bieb}{\textsf{\sc Bieber}}
\newcommand{\FS}{\textsf{\sc FibSwift}}
\newcommand{\Swift}{\textsf{\sc Swift}}

%%%%%%%%%%%%%% Capsule %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\capsule}[2]{\vspace{0.5em}
  \shadowbox{%
    \begin{minipage}{.90\linewidth}%
      \textbf{#1:}~#2%
    \end{minipage}}
  \vspace{0.5em} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcounter{ques}
\newenvironment{question}{\stepcounter{ques}{\noindent\bf Question \arabic{ques}:}}{\vspace{5mm}}

\begin{document} 

\begin{center} \Large\bf
COMP 3105 --- Fall 2025 --- Assignment 1
\end{center} 

\begin{center}
Pascal Law 101318299 $\vert$ Ayaan Aleem 101307254
\end{center}

\begin{question}

\qquad

\qquad

Subquestion b1.

\quad

\[
\underset{1\times(d+1)}{\mathbf{c}^T}
\;\;
\underset{(d+1)\times 1}{\mathbf{u}}
\;=\; \delta
\]

Since $\mathbf{u} =
\begin{bmatrix}
\mathbf{w} \\
\delta
\end{bmatrix}
$, we can make all the weights in $\mathbf{w}$ zero, and just have $\delta$ in the last entry of $\mathbf{u}$.

As such, we can set:
\[
\mathbf{c} =
\underset{(d+1)\times 1}{
\begin{bmatrix}
\textbf{0}_{1 \times d} & 1
\end{bmatrix}}^{T}
\]

\quad

Subquestion b2. 

\quad

We can break
\[
\underset{1\times (d+1)}{
G^{1}
}
\begin{bmatrix}
\textbf{w} \\
\delta
\end{bmatrix}
\le
\underset{1\times 1}{\textbf{h}^{1}}
\]
into two parts, and it becomes:

\[
\underset{1\times d}{G^{11}}
\underset{1\times 1}{G^{12}}
\begin{bmatrix}
\textbf{w} \\
\delta
\end{bmatrix}
\le
\underset{1\times 1}{\textbf{h}^{1}}
\]

As such,
\[
\underset{1\times d}{G^{11}}
\underset{d\times 1}{\textbf{w}}
+
\underset{1\times 1}{G^{12}}
\delta
\le
\underset{1\times 1}{\textbf{h}^{1}}
\]

Thus, given that the first part of the linear programming problem is:
\[-\delta \le 0\]
we can let: \[
\underset{1\times d}{G^{11}}
= \textbf{0}_{1 \times d}
\]
\[
\underset{1\times 1}{G^{12}}
= -1
\]
\[
\underset{1\times (d+1)}{
G^{1}
}
=
\underset{1\times (d+1)}{
\begin{bmatrix}
\textbf{0}_{1 \times d} & -1
\end{bmatrix}}
\]
\[
\underset{1\times 1}{\textbf{h}^{1}}
= 0\]

\quad

Subquestion b3. 

\quad

We can break
\[
\underset{n\times (d+1)}{
G^{2}
}
\begin{bmatrix}
\textbf{w} \\
\delta
\end{bmatrix}
\le
\underset{n\times 1}{\textbf{h}^{2}}
\]
into two parts, and it becomes:

\[
\underset{n\times d}{G^{21}}
\underset{n\times 1}{G^{22}}
\begin{bmatrix}
\textbf{w} \\
\delta
\end{bmatrix}
\le
\underset{n\times 1}{\textbf{h}^{2}}
\]

As such,
\[
\underset{n\times d}{G^{21}}
\underset{d\times 1}{\textbf{w}}
+
\underset{n\times 1}{G^{22}}
\delta
\le
\underset{n\times 1}{\textbf{h}^{2}}
\]

Thus, given that the second part of the linear programming problem is:
\[X \textbf{w} - \delta \cdot \textbf{1}_n \le \textbf{y}\]
we can let: \[
\underset{n\times d}{G^{21}}
= X
\]
\[
\underset{n\times 1}{G^{22}}
= -\textbf{1}_{n \times 1}
\]
\[
\underset{n\times (d+1)}{
G^{2}
}
=
\underset{n\times (d+1)}{
\begin{bmatrix}
X & -\textbf{1}_{n \times 1}
\end{bmatrix}}
\]
\[
\underset{n\times 1}{\textbf{h}^{2}}
= \textbf{y}\]

\quad

Subquestion b4. 

\quad

We can break
\[
\underset{n\times (d+1)}{
G^{3}
}
\begin{bmatrix}
\textbf{w} \\
\delta
\end{bmatrix}
\le
\underset{n\times 1}{\textbf{h}^{3}}
\]
into two parts, and it becomes:

\[
\underset{n\times d}{G^{31}}
\underset{n\times 1}{G^{32}}
\begin{bmatrix}
\textbf{w} \\
\delta
\end{bmatrix}
\le
\underset{n\times 1}{\textbf{h}^{3}}
\]

As such,
\[
\underset{n\times d}{G^{31}}
\underset{d\times 1}{\textbf{w}}
+
\underset{n\times 1}{G^{32}}
\delta
\le
\underset{n\times 1}{\textbf{h}^{3}}
\]

Thus, given that the third part of the linear programming problem is:
\[-X \textbf{w} - \delta \cdot \textbf{1}_n \le -\textbf{y}\]
we can let: \[
\underset{n\times d}{G^{31}}
= -X
\]
\[
\underset{n\times 1}{G^{32}}
= -\textbf{1}_{n \times 1}
\]
\[
\underset{n\times (d+1)}{
G^{3}
}
=
\underset{n\times (d+1)}{
\begin{bmatrix}
-X & -\textbf{1}_{n \times 1}
\end{bmatrix}}
\]
\[
\underset{n\times 1}{\textbf{h}^{3}}
= -\textbf{y}\]

\quad

Subquestion c1.

\begin{center}
Table 1: Different training losses for different models
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cc}
    \textbf{Model} & $L_2$ loss & $L_\infty$ loss \\
    \hline
    $L_2$ model   &      0.09557022      &         1.58819212        \\
    $L_\infty$ model &      0.23302085      &           0.86917599      
\end{tabular}
\end{table}

\begin{center}
Table 2: Different test losses for different models
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cc}
    \textbf{Model} & $L_2$ loss & $L_\infty$ loss \\
    \hline
    $L_2$ model   &      0.05291075      &        1.03171424         \\
    $L_\infty$ model &      0.29564879      &         2.1535331        
\end{tabular}
\end{table}

\quad

Subquestion c2.

Analysis of result:

When we compare our training data with our test data, we find that our $L_2$ loss has a smaller difference when compared to our $L_{\infty}$ loss, and similarly our $L_2$ model also has a smaller difference when compared to our $L_{\infty}$ model.

This means that our $L_2$ model using $L_2$ loss has the smallest difference and so is the most accurate prediction of the data. On the other hand, the $L_{\infty}$ model has the biggest difference which means that $L_{\infty}$ model using $L_{\infty}$ loss is the most inaccurate prediction of the data.

The reason that the $L_{\infty}$ loss is so inaccurate is because of the difference of size between the test data and the training data. We only generate 30 samples for the training while we generate 1000 samples for the test data. This means that our $L_{\infty}$ loss is modelled on a very limited set of training data, and so it is heavily affected by outliers in our much larger test data.

This is in contrast with our $L_2$ loss, which is not as affected by outliers, which makes it much more accurate when the training data sample is small.

When considering the models, the $L_2$ model is more accurate because it has a smaller tolerance, which means it doesn't chase outliers like our $L_{\infty}$ model.

These factors lead to the conclusion that our $L_2$ loss and $L_2$ model are more accurate than the $L_{\infty}$ counterparts for this set of training and test data.

\quad

\quad

\quad

Subquestion d2.

\begin{center}
Table 3: Different training losses for different models
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cc}
    \textbf{Model} & $L_2$ loss & $L_\infty$ loss \\
    \hline
    $L_2$ model   &      53.18368801      &         32.21643524        \\
    $L_\infty$ model &      66.09571725      &           26.39169568      
\end{tabular}
\end{table}

\begin{center}
Table 4: Different test losses for different models
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{c|cc}
    \textbf{Model} & $L_2$ loss & $L_\infty$ loss \\
    \hline
    $L_2$ model   &      55.20252554      &        33.40449987         \\
    $L_\infty$ model &      68.04960819      &         36.9655386        
\end{tabular}
\end{table}

\end{question}

\newpage

\begin{question}

Subquestion c1.

\begin{center}
Table 5: Training accuracies with different hyper-parameters
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{c c|c c|c c}
    \textbf{m} & \textbf{Train Accuracy} & \textbf{dim1} & \textbf{Train Accuracy} & \textbf{dim2} & \textbf{Train Accuracy} \\
    \hline
    10  & 0.9725  & 1  & 0.8467  & 1  & 0.9266 \\
    50  & 0.9252  & 2  & 0.9262  & 2  & 0.9275 \\
    100 & 0.9217  & 4  & 0.98285  & 4  & 0.9255 \\
    200 & 0.923125  & 8  & 0.99935  & 8  & 0.9349 \\
\end{tabular}
\end{table}

\begin{center}
Table 6: Testing accuracies with different hyper-parameters
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{c c|c c|c c}
    \textbf{m} & \textbf{Test Accuracy} & \textbf{dim1} & \textbf{Test Accuracy} & \textbf{dim2} & \textbf{Test Accuracy} \\
    \hline
    10  & 0.882065  & 1  & 0.836885  & 1  & 0.91926 \\
    50  & 0.911655  & 2  & 0.91675  & 2  & 0.915435 \\
    100 & 0.91711  & 4  & 0.968755  & 4  & 0.915865 \\
    200 & 0.91836  & 8  & 0.996295  & 8  & 0.90904 \\
\end{tabular}
\end{table}

Subquestion c2.

Our m represents the number of data points in the training data sample. As our m increases, our training accuracy decreases, but our test accuracy increases. This is because when m is low, we have very few data points to train our optimal parameter which causes us to create a line that is unsuitable for a large number of data points, like the amount we have in the test data. On the other hand, as our m increases, we have a much larger sample to train our optimal parameter on, which helps to create a line that is more suitable for our test data points. However, this may not necessarily be completely accurate for the set of training data points due to the higher amount of randomly distributed points that may skew the model.

Both for training and test data, as dim1 increases our accuracy becomes almost perfect. This is because dim1 is an expression of the number of useful dimensions; when we have a higher dim1 in training, our info on the data is also higher which allows us to predict labels with a higher degree of accuracy. Similarly, when our test data has a higher dim1, our accuracy is again higher because of more useful information on the data points.

However, as our dim2 increases, it has opposite effects in training and test data; in training data, it marginally increases the accuracy whereas in test data, the accuracy drops slowly but steadily. This is because dim2 is an expression of the number of noise dimensions which add to the variance. When we train using a larger dim2, it doesn't really increase our accuracy by much, however when we use test data with larger dim2, the increase in variance negatively affects our accuracy.

\end{question}

\newpage

\textbf{References}:

\quad

\begin{itemize}

\item Asked on Discord group about an error message related to \texttt{scipy.optimize.minimize} method.

\item Checked Discord group messages about the error messages which  others have encountered (especially \texttt{scipy.special.expit()} method and how to suppress warnings).

\item Asked ChatGPT to explain dim1 and dim2 meanings in Q2.

\item Asked ChatGPT to explain Python syntax as in \texttt{for i, m in enumerate((10, 50, 100, 200)):}

\item Googled numpy, pandas, scipy, autograd, etc. documentations.
\end{itemize}

\end{document} 
